{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to huggingface\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt -qq install git-lfs\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "def show_images(x):\n",
    "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
    "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
    "    grid = torchvision.utils.make_grid(x)\n",
    "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
    "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
    "    return grid_im\n",
    "\n",
    "\n",
    "def make_grid(images, size=64):\n",
    "    \"\"\"Given a list of PIL images, stack them together into a line for easy viewing\"\"\"\n",
    "    output_im = Image.new(\"RGB\", (size * len(images), size))\n",
    "    for i, im in enumerate(images):\n",
    "        output_im.paste(im.resize((size, size)), (i * size, 0))\n",
    "    return output_im\n",
    "\n",
    "\n",
    "# Mac users may need device = 'mps' (untested)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Viable Pipeline\n",
    "\n",
    "Huggingface Diffusers is divided into three main components:\n",
    "- **Pipelines**: high-level classes designed to rapidly generate samples from popular trained diffusion models in a user-friendly fashion.\n",
    "- **Models**: popular architectures for training new diffusion models, e.g. UNet.\n",
    "- **Schedulers**: various techniques for generating images from noise during inference as well as to generate noisy images for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import denoising diffusion probabilistic models (see https://arxiv.org/abs/2006.11239)\n",
    "from diffusers import DDPMPipeline\n",
    "\n",
    "# load the butterfly ddpm model pipeline\n",
    "butterfly_pipeline = DDPMPipeline.from_pretrained(\"johnowhitaker/ddpm-butterflies-32px\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch of images\n",
    "images = butterfly_pipeline(batch_size=8).images\n",
    "\n",
    "# show the images\n",
    "make_grid(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DDPM\n",
    "\n",
    "**Study case**: Train using [1000 Butterfly datasets](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# load the dataset\n",
    "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# augmentation transforms\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def do_transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "# apply the transform to the dataset\n",
    "dataset.set_transform(do_transform)\n",
    "\n",
    "# create a dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first batch of images\n",
    "xb = next(iter(train_dataloader))[\"images\"].to(device)[:8]\n",
    "print(\"X shape:\", xb.shape)\n",
    "show_images(xb).resize((8 * 64, 64), resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add noise repeatedly to the input images to train the model. So we can use the **DDPMScheduler** to add noise to the input images. This method is **instant without thinking about mathematica**l calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DDPM paper describes a corruption process that adds a small amount of noise for every `timestep`. \n",
    "\n",
    "$q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quad\n",
    "q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})$<br><br>\n",
    "\n",
    "Where $x_t-1$ is the previous timestep, $x_t$ is the current timestep, and $\\beta_t$ is the timestep-dependent noise level.\n",
    "\n",
    "**Flow**:\n",
    "- Take $x_t-1$ as input\n",
    "- Scale it by $\\sqrt{1 - \\beta_t}$\n",
    "- Add noise $\\beta_t\\mathbf{I}$\n",
    "- Output $x_t$\n",
    "\n",
    "But, we don't want to do this operation $t$ times to get the final image so we have another formula to get $x_t$ directly from $x_0$.\n",
    "\n",
    "$\\begin{aligned}\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, {(1 - \\bar{\\alpha}_t)} \\mathbf{I})\n",
    "\\end{aligned}$ where $\\bar{\\alpha}_t = \\prod_{i=1}^T \\alpha_i$ and $\\alpha_i = 1-\\beta_i$<br><br>\n",
    "\n",
    "Where:\n",
    "- $x_0$ is the initial image\n",
    "- $x_t$ is the final image\n",
    "- $\\bar{\\alpha}_t$ is the final noise level\n",
    "- $\\sqrt{\\bar{\\alpha}_t}$ is how the **original image is scaled**; When this value is 0, the image is completely random noise, and when it is 1, the image is the same as the input image.\n",
    "- $\\sqrt{\\bar{\\alpha}_t - 1}$ is how many **noise added to the image**; When this value is 0, the image is the same as the input image, and when it is 1, the image is completely random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(noise_scheduler.alphas_cumprod.cpu() ** 0.5, label=r\"${\\sqrt{\\bar{\\alpha}_t}}$\")\n",
    "plt.plot((1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5, label=r\"$\\sqrt{(1 - \\bar{\\alpha}_t)}$\")\n",
    "plt.legend(fontsize=\"x-large\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the **two plots are meeting** at the same point, then the contibution of the original image ($x_0$) and the noise is **equal** in the combined image ($x_t$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just random things to try:\n",
    "# One with too little noise added:\n",
    "noise_scheduler_temp = DDPMScheduler(num_train_timesteps=1000, beta_start=0.001, beta_end=0.004)\n",
    "\n",
    "plt.plot(noise_scheduler_temp.alphas_cumprod.cpu() ** 0.5, label=r\"${\\sqrt{\\bar{\\alpha}_t}}$\")\n",
    "plt.plot((1 - noise_scheduler_temp.alphas_cumprod.cpu()) ** 0.5, label=r\"$\\sqrt{(1 - \\bar{\\alpha}_t)}$\")\n",
    "plt.legend(fontsize=\"x-large\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'cosine' schedule, which may be better for small image sizes:\n",
    "noise_scheduler_temp = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')\n",
    "\n",
    "plt.plot(noise_scheduler_temp.alphas_cumprod.cpu() ** 0.5, label=r\"${\\sqrt{\\bar{\\alpha}_t}}$\")\n",
    "plt.plot((1 - noise_scheduler_temp.alphas_cumprod.cpu()) ** 0.5, label=r\"$\\sqrt{(1 - \\bar{\\alpha}_t)}$\")\n",
    "plt.legend(fontsize=\"x-large\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate adding noise to the batch\n",
    "timesteps = torch.linspace(0, 999, 8).long().to(device)\n",
    "noise = torch.randn_like(xb)\n",
    "noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps)\n",
    "\n",
    "print(f\"Timesteps: {timesteps}\")\n",
    "print(f\"Noise shape: {noise.shape}\")\n",
    "print(f\"Noisy Xb shape: {noisy_xb.shape}\")\n",
    "show_images(noise).resize((8 * 64, 64), resample=Image.NEAREST)\n",
    "show_images(noisy_xb).resize((8 * 64, 64), resample=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most diffusion models are based on the [UNet](https://arxiv.org/abs/1505.04597) architecture.\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/unet-model.png)\n",
    "\n",
    "**Flow**:\n",
    "\n",
    "- The model has the input image go through several blocks of ResNet layers, each of which halves the image size by 2\n",
    "- Then through the same number of blocks that upsample it again\n",
    "- There are skip connections linking the features on the downsample path to the corresponding layers in the upsample path\n",
    "\n",
    "\n",
    "This model will predict images with **same size** as the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "# create a UNet model\n",
    "model = UNet2DModel(\n",
    "    sample_size=IMAGE_SIZE, # target image size\n",
    "    in_channels=3, # number of input channels\n",
    "    out_channels=3, # number of output channels\n",
    "    layers_per_block=2, # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(64, 128, 256, 512), # number of output channels for each UNet block; More channels -> more parameters\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", # regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\", # ResNet downsampling block with spatial self-attention\n",
    "        \"AttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"AttnUpBlock2D\", # ResNet upsampling block with spatial self-attention\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\", # regular ResNet upsampling block\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Higher-resolution images may require more down and up-blocks\n",
    "- Keep the attention layers only at the lowest resolution to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_prediction = model(noisy_xb, timesteps).sample\n",
    "model_prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Flow:\n",
    "- Get each batch of images\n",
    "- Sample some random timesteps\n",
    "- Noise the data accordingly\n",
    "- Feed the noisy data through the model\n",
    "- Compare the model predictions with the target (noise) using MSE as the loss function\n",
    "- Update the model parameters via `loss.backward()` and `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the noise scheduler\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\"\n",
    ")\n",
    "\n",
    "# training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "\n",
    "        # sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # sample a random timestep for each image in the batch\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device\n",
    "        )\n",
    "\n",
    "        # add noise to the images according to the noise magnitude at the sampled timestep\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # get the model prediction\n",
    "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # update the model parameters with the optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss_last_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].plot(losses)\n",
    "axs[1].plot(np.log(losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to instead load the model I trained earlier:\n",
    "# model = butterfly_pipeline.unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Process (Generate Images)\n",
    "\n",
    "### Option 1: Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "image_pipe = DDPMPipeline(\n",
    "    unet=model,\n",
    "    scheduler=noise_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_output = image_pipe()\n",
    "pipeline_output.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pipeline\n",
    "image_pipe.save_pretrained(\"butterfly_ddpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Writing a Sampling Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow:\n",
    "- Begin with random noise\n",
    "- Run through the scheduler timesteps from most to least noisy\n",
    "- Removing a small amount of noise at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random starting point (8 random images):\n",
    "sample = torch.randn(8, 3, 32, 32).to(device)\n",
    "\n",
    "for i, t in enumerate(noise_scheduler.timesteps):\n",
    "\n",
    "    # get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = model(sample, t).sample\n",
    "\n",
    "    # update sample with step\n",
    "    sample = noise_scheduler.step(residual, t, sample).prev_sample\n",
    "\n",
    "show_images(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Hardware usage:\n",
    "RTX 2060 6GB\n",
    "CPU: 115%\n",
    "Memory: 1639MB\n",
    "GPU: 92%\n",
    "GPU Memory: 3794MB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Model to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"butterfly-ddpm-32\"\n",
    "hub_model_id = get_full_repo_name(model_name)\n",
    "hub_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "create_repo(hub_model_id)\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"butterfly_ddpm/scheduler\", path_in_repo=\"\", repo_id=hub_model_id\n",
    ")\n",
    "api.upload_folder(folder_path=\"butterfly_ddpm/unet\", path_in_repo=\"\", repo_id=hub_model_id)\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"butterfly_ddpm/model_index.json\",\n",
    "    path_in_repo=\"model_index.json\",\n",
    "    repo_id=hub_model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard\n",
    "\n",
    "content = f\"\"\"\n",
    "---\n",
    "license: mit\n",
    "tags:\n",
    "- pytorch\n",
    "- diffusers\n",
    "- unconditional-image-generation\n",
    "- diffusion-models-class\n",
    "---\n",
    "\n",
    "# Model Card for Unit 1 of the [Diffusion Models Class 🧨](https://github.com/huggingface/diffusion-models-class)\n",
    "\n",
    "This model is a diffusion model for unconditional image generation of cute 🦋.\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from diffusers import DDPMPipeline\n",
    "\n",
    "pipeline = DDPMPipeline.from_pretrained('{hub_model_id}')\n",
    "image = pipeline().images[0]\n",
    "image\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard(content)\n",
    "card.push_to_hub(hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "image_pipe = DDPMPipeline.from_pretrained(hub_model_id).to(device)\n",
    "pipeline_output = image_pipe(batch_size=8)\n",
    "make_grid(pipeline_output.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If error:\n",
    "```\n",
    "An error occurred while trying to fetch /home/sugab/.cache/huggingface/hub/models--hiseulgi--butterfly-ddpm-32/snapshots/be8922159e3c7177c4573627a25ea3b28d074720: Error no file named diffusion_pytorch_model.safetensors found in directory /home/sugab/.cache/huggingface/hub/models--hiseulgi--butterfly-ddpm-32/snapshots/be8922159e3c7177c4573627a25ea3b28d074720.\n",
    "\n",
    "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
    "```\n",
    "\n",
    "You need to manually download the `*.safe_tensors` file from the huggingface repository and place it in the cache directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "- [Denoising Diffusion Probabilistic Models - Paper](https://arxiv.org/abs/2006.11239)\n",
    "- [What are Diffusion Models? - Youtube](https://www.youtube.com/watch?v=fbLgFrlTnGU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
